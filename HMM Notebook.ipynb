{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HMM \n",
    "\n",
    "An HMM Part-of-Speech (POS) Tagger is a statistical model that assigns parts of speech to words in a sentence using the Hidden Markov Model (HMM) framework.\n",
    "\n",
    "First we import our model with the needed dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "from conllu_dataloader import *\n",
    "from model.hmm import HMMPOSTagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we initialize the model we have these parameters: \n",
    "\n",
    "- `tags`: set of possible tags\n",
    "- `vocab`: set of possible words\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "hmm = HMMPOSTagger(tags, vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After initializing we get the following attributes:\n",
    "\n",
    "- `self.tags2idx` = dictionary to get the idx from the tags\n",
    "- `self.idx2tags` = dictionary to get the tags from the idx\n",
    "- `self.tags` = set of used tags\n",
    "- `self.Q`= number of tags\n",
    "- `self.vocab` = vocabulary\n",
    "- `self.transition_counts` = matrix to store the transition counts\n",
    "- `self.emission_counts` = matrix to store the emission counts\n",
    "- `self.transition_probs` = matrix to store the transition probabilities\n",
    "- `self.emission_probs` = matrix to store the emission probabilities\n",
    "- `self.word_counts` = counter of each word\n",
    "- `self.tag_counts` = counter of each tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONLLU DATALOADER\n",
    "\n",
    "Converts conllu files to csv. It generates 4 files: sentences, sentences post tags, general tags, vocabulary\n",
    "\n",
    "# Loader class: functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN FUNCTION\n",
    "\n",
    "Function to train our HMM model\n",
    "\n",
    "Our train function has the following parameters:\n",
    "- `sentences`\n",
    "list of all the sentences in our train dataset\n",
    "- `pos_tags`\n",
    "list of all the PoS tags of the sentences in our train dataset\n",
    "- `change_vocab` if true, it updates the vocab by removing the least frequent words and turning them all to <UNK>\n",
    "\n",
    "First the model counts the occurrences of the tags and the words and stores them in two dictionaries. \n",
    "The first one is \"transition_counts\" where we store each tags that follows another tag; and the second one is \"emission_counts\" where we store each word associated with a tag.\n",
    "We also take into account the number of occurrences of each tag and word, and we store them in \"tag_counts\" and \"word_counts\" respectively.\n",
    "\n",
    "Every time we start a new sentence we insert a \"*\" in the first position and a <STOP> in the last position. This is to take into consideration the first and last words of a sentence. \n",
    "\n",
    "After running all the sentences we check if we have to change the vocabulary. If so, we revise the words in dictionary \"word_counts\", we remove the words that appear less than 5 times and we add the counts to the <UNK> token. We do this so that the model can work better with unknown words.\n",
    "\n",
    "Then, to sample with the model, we have to get the probabilities of transitions and emission counts. This is achieved with the following function:\n",
    "$$\n",
    "P(q_i|q_{i-1})={count(q_{i-1},q_i) \\over count(q_{i-1})}\n",
    "$$\n",
    "\n",
    "Where $ q_{i-1} $ are going to be the previous tags and $ q_i $ the current tags in transition counts, and with emission counts, the $ q_{i-1} $ are going to be the current words and the $q_i$ are going to be the current tags. \n",
    "\n",
    "\n",
    "After running the train function we will have two matrices:\n",
    " 1) transition probabilty matrix\n",
    " 2) emission probabilty matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "hmm.train(sentences, pos_tags, change_vocab = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to inspect the transition and emission probabilities we can check them like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(hmm.transition_probs)\n",
    "print(hmm.emission_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VITERBI ALGORITHM\n",
    "\n",
    "This algorithm determines what PoS tags a sentence contains using the HMM that we have trained. \n",
    "The parameter that we use here is:\n",
    "- `sentence`: the sentence we have to predict the PoS tags of\n",
    "\n",
    "\n",
    "First, we take a look at the sentence and we search each word in the vocabulary. If there is no such word in the vocabulary we replace it with the <UNK> token.\n",
    "Next, we apply the algorithm. To do that, we define two variables: the viterbi matrix, where we will store the probabilities; and the backpointer, where we store the most probable path. At this point we calculate the probabilty of the word for each tag with the following function: $$viterbi[q,t]=\\max viterbi[q',t-1]*A_{[q',q]}*B_{[q,t]}$$\n",
    "\n",
    "Where $A$ is the transition matrix and $B$ the mission matrix.\n",
    "\n",
    "The $q'$ that gets the maximum pobabilty is stored in the backpointer. This way the viterbi function gives back the  initial sentence and the PoS tags predicted by the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "sentence = ['Jeremy','loves','NLP']\n",
    "hmm.viterbi_alg(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATE FUNCTION\n",
    "\n",
    "This function evaluates the test dataset with Viterbi and, with whatever Viterbi returns, we calculate accuracy and F1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "test = [['Jeremy', 'Loves', 'NLP'],\n",
    "        ['How', 'do', 'people', 'live', 'in', 'houses'],\n",
    "        ['Those','in', 'power', 'have', 'little', 'interest', 'in', 'education']\n",
    "]\n",
    "tags = [['NOUN', 'VERB', 'NOUN'],\n",
    "        ['ADV', 'VERB', 'NOUN', 'VERB', 'ADP', 'NOUN'],\n",
    "        ['PRON', 'ADP', 'NOUN', 'VERB', 'ADJ', 'NOUN', 'ADP', 'NOUN']]\n",
    "        \n",
    "hmm.evaluate(test, tags)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
